#std-lib
import time
import random
import typing

import streamlit as st


def get_llm_response(prompt: str) -> str:
    """
    Simula uma chamada de API para um Large Language Model.

    Args:
        prompt: O texto de entrada do usu√°rio.

    Returns:
        Uma resposta de texto gerada pelo "modelo".
    """
    print(f"DEBUG: Enviando para a API (simulado): '{prompt}'")
    
    # API do Google Gemini (requer 'pip install google-generativeai')
    #
    # import google.generativeai as genai
    #
    # genai.configure(api_key="SUA_API_KEY_AQUI")
    # model = genai.GenerativeModel('gemini-pro')
    # try:
    #     response = model.generate_content(prompt)
    #     return response.text
    # except Exception as e:
    #     print(f"Ocorreu um erro na API: {e}")
    #     return "Desculpe, n√£o consegui processar sua solicita√ß√£o no momento."

    time.sleep(random.uniform(1, 2.5))
    
    mock_responses = [
        f"Esta √© uma resposta simulada para a sua pergunta sobre '{prompt}'. Em um ambiente real, eu me conectaria a uma API de LLM.",
        "Processando sua solicita√ß√£o... Ah, lembrei que sou apenas uma simula√ß√£o! Mas se eu fosse real, daria uma resposta incr√≠vel.",
        f"Interessante voc√™ perguntar sobre '{prompt}'. O integrador de API est√° funcionando, mas est√° configurado para retornar esta mensagem de teste.",
        "Para conectar a uma API real, edite a fun√ß√£o `get_llm_response` neste script Python. As instru√ß√µes est√£o nos coment√°rios do c√≥digo."
    ]
    
    return random.choice(mock_responses)

st.set_page_config(page_title="Meu Gemini", page_icon="ü§ñ")

st.title("Dakila IA")
st.caption("Interface de comunica√ß√£o para a Dakila IA")

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ol√°! Como posso te ajudar hoje?"}
    ]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Digite sua mensagem..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
        
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            response = get_llm_response(prompt)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            
            st.markdown(response)